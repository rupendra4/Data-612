---
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
   
---

<div style="text-align:center;"> 
  <h1><B>DATA 612 - Project 3 : Matrix Factorization methods </B></h1>
  <h3><B>Bikash Bhowmik, Rupendra Shrestha</B></h3> 
  <h4><B>26 Jun 2025</B></h4>
</div>


Column {data-width=150}
-----------------------------------------------------------------------

### 
<a href="#mySection1" style="font-size: 1em; font-weight: bold;" >Instruction</a>
<br>
<a href="#mySection2" style="font-size: 1em; font-weight: bold;" >Introduction</a>
<br>
<a href="#mySection3" style="font-size: 1em; font-weight: bold;" >Load Data</a>
<br>
<a href="#mySection4" style="font-size: 1em; font-weight: bold;" >Show Source Data in Data Frame as Table</a>
<br>
<a href="#mySection5" style="font-size: 1em; font-weight: bold;" >Data Transformation</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection51" style="font-size: 1em; " >Combine Data</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection52" style="font-size: 1em; " >Create Matrix</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection53" style="font-size: 1em; " >Build and Train Data Models</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection54" style="font-size: 1em; " >Build and Test SVD, Funk SVD, and ALS recommendation algorithms</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection55" style="font-size: 1em; " >User Based Collaborative Filtering (UBCF) Model</a>
<br>
<a href="#mySection6" style="font-size: 1em; font-weight: bold;" >Data Visualization</a>
<br>
<a href="#mySection7" style="font-size: 1em; font-weight: bold;" >Singular Value Decomposition (SVD) Model</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection71" style="font-size: 1em; " >How SVD Connects to Recommender Systems</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection72" style="font-size: 1em; " >SVD Model Implementation</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection73" style="font-size: 1em; " >Run-Times</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection74" style="font-size: 1em; " >Predictions</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection75" style="font-size: 1em; " >Manual Singular Value Decomposition</a>
<br>
<a href="#mySection8" style="font-size: 1.5em; font-weight: bold;">Empirical Results:</a>
<br>
<a href="#mySection9" style="font-size: 1em; font-weight: bold;" >Conclusion</a>
<br>


Column {data-width=875}
-----------------------------------------------------------------------
### 
<a id="mySection1"></a> 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Load all required packages
library(tidyverse)
library(readr)
library(sqldf)
library(dplyr)
library(tidyr)
library(tinytex)
library(recommenderlab)
library(kableExtra)
library(gridExtra)
```

<font size="4">
<B>
Instruction
</B>
<font>

The goal of this assignment is give you practice working with Matrix Factorization techniques.

Your task is implement a matrix factorization method-such as singular value decomposition (SVD) or Alternating Least Squares (ALS)-in the context of a recommender system.

+ SVD can be thought of as a pre-processing step for feature engineering. You might easily start with thousands or millions   of items, and use SVD to create a much smaller set of “k” items (e.g. 20 or 70).
+ SVD builds features that may or may not map neatly to items (such as movie genres or news topics). As in many areas of   
  machine learning, the lack of explainability can be an issue).
+ SVD requires that there are no missing values. There are various ways to handle this, including

1. imputation of missing values,
2. mean-centering values around 0, or
3. using a more advance technique, such as stochastic gradient descent to simulate SVD in populating the factored matrices.

+ Calculating the SVD matrices can be computationally expensive, although calculating ratings once the factorization is completed is very fast. You may need to create a subset of your data for SVD calculations to be successfully performed, especially on a machine with a small RAM footprint.

<a id="mySection2"></a>
<font size="4">
<B>
Introduction
</B>
<font>

We cover matrix factorization techniques for building recommendation systems. Of specific interest in the project are Singular Value Decomposition (SVD), Funk SVD, and Alternating Least Squares (ALS) - three of the most widely used techniques for generating personalized recommendations.

With the MovieLens dataset that we use to store user ratings for various movies, we demonstrate how these factorization models lower-dimensionalize the user-item rating matrix into representations that capture latent factors. The project involves several key steps: dataset cleaning and preparation, execution of the recommendation algorithms with the recommenderlab package, performance measurement using accuracy metrics, and visualization of data and results.

The objective is to understand the trade-offs between different factorization methods in terms of accuracy, efficiency, interpretability, and scalability, and to compare them with baseline collaborative filtering methods like UBCF. The experiential project sets the boundaries on how matrix factorization is at the heart of most modern recommendation systems.

<a id="mySection3"></a>
<font size="4">
<B>
Load Data
</B>
<font>

We load and explore the MovieLens dataset, which includes user ratings and movie information. We primarily use ratings.csv and movies.csv to build the user-item rating matrix. These files are merged to link movie titles and genres with corresponding user ratings, providing the foundation for matrix factorization and recommendation modeling.

The dataset consists of the following files:

movies.csv – Contains movieId, title, and genres for each movie.

ratings.csv – Includes user-generated ratings for movies, along with userId, movieId, and timestamp


<a id="mySection4"></a>
<font size="4">
<B>
Show Source Data in Data Frame as Table
</B>
<font>

We present a preview of the raw data from movies.csv and ratings.csv using both SQL queries and formatted tables. This helps us understand the structure of the dataset, including movie titles, genres, user IDs, and rating values, which are essential for building the recommender system.

```{r }
# Load necessary libraries
library(sqldf)
library(readr)

# Read the CSV files
Ratings <- read_csv("Ratings.csv")
Movies <- read_csv("Movies.csv")
```

Movies

```{r }
sqldf("SELECT 
  SUBSTR(movieId, 1, 4) AS title,
  SUBSTR(title, 1, 30) AS genre,
  SUBSTR(genres, 1, 30) AS description
FROM Movies
LIMIT 10;") 

```


```{r }
kable(head(Movies,10)) %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed","responsive"),
  full_width   = F,position = "left",font_size = 12) %>%
  row_spec(0, background ="gray")

```

Ratings

```{r }
sqldf("select * from Ratings LIMIT 10") 
```

Kable

```{r }
kable(head(select(Ratings, userId:rating), 10)) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```


<a id="mySection5"></a>
<font size="4">
<B>
Data Transformation
</B>
<font>


<a id="mySection51"></a>
<font size="4">
<B>
Combine Data
</B>
<font>

Create a “movie_ratings” from movies and corresponding ratings data frames

```{r }
movie_ratings <- merge(Ratings, Movies, by="movieId")
kable(head(movie_ratings, 10)) %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

By merging the ratings and movies data frames on movieId, we created a unified dataset (movie_ratings) that links each rating to its corresponding movie title and genre. This combined view is crucial for performing both matrix factorization and generating meaningful movie recommendations.


<a id="mySection52"></a>
<font size="4">
<B>
Create Matrix
</B>
<font>


Create a “movieMatrix”

```{r }
Ratings_filtered <- Ratings %>%
  group_by(userId) %>%
  filter(n() > 50)
# Merge again and recreate the matrix
movieSpread <- Ratings_filtered %>% 
  select(-timestamp) %>% 
  spread(movieId, rating)

# Fix row names assignment
row.names(movieSpread) <- as.character(movieSpread$userId)

# Drop userId column and convert to matrix
movieMatrix <- as.matrix(movieSpread[,-1])

# Convert to realRatingMatrix
movieRealMatrix <- as(movieMatrix, "realRatingMatrix")

```
We filtered out users with fewer than 50 ratings to reduce sparsity and improve model performance. The data was then reshaped into a user-item rating matrix (movieMatrix) and converted into a realRatingMatrix format (movieRealMatrix), which is required by the recommenderlab package for building recommendation models.


<a id="mySection53"></a>
<font size="4">
<B>
Build and Train Data Models
</B>
<font>

This step involves preparing the dataset and developing predictive models. First, the data is split into training and testing sets to evaluate model performance fairly. Then, various machine learning algorithms are trained on the training data to learn patterns. 
In order to test any models, we need to split our data into training and testing sets with 80/20 ratio.

```{r }
# Test ALS only on a reduced set
set.seed(100)
eval_small <- evaluationScheme(movieRealMatrix, method = "split", train = 0.8, given = 20, goodRating = 3)
train_small <- getData(eval_small, "train")

# Train ALS
r.als <- Recommender(train_small, method = "ALS")

# Predict
known_small <- getData(eval_small, "known")
p.als <- predict(r.als, known_small, type = "ratings")

# Remove users with all NA
movieMatrix <- movieMatrix[rowSums(is.na(movieMatrix)) != ncol(movieMatrix), ]

# Remove movies (columns) with all NA
movieMatrix <- movieMatrix[, colSums(is.na(movieMatrix)) != nrow(movieMatrix)]

# Then convert
movieRealMatrix <- as(movieMatrix, "realRatingMatrix")

```

The dataset was split into training and testing sets using an 80/20 ratio. The Alternating Least Squares (ALS) model was trained using the training data. To ensure the rating matrix was suitable for modeling, rows and columns with entirely missing values were removed. The cleaned matrix was then re-converted into a realRatingMatrix, enabling efficient and accurate model training and prediction.


<a id="mySection54"></a>
<font size="4">
<B>
 Build and Test SVD, Funk SVD, and ALS recommendation algorithms
</B>
<font>

This section builds and evaluates three matrix factorization-based recommender algorithms: SVD, Funk SVD (SVDF), and ALS. Each model is trained using the training dataset and then used to predict ratings on the test dataset. The predicted ratings are examined by extracting a sample rating matrix. 

```{r }
# Create the recommender based on SVD and SVDF using the training data
r.svd <- Recommender(getData(eval_small, "train"), "SVDF")
r.svdf <- Recommender(getData(eval_small, "train"), "SVD")
# ALS was already trained separately above as r.als

# Compute predicted ratings for test data
p.svd <- predict(r.svd, getData(eval_small, "known"), type = "ratings")
p.svdf <- predict(r.svdf, getData(eval_small, "known"), type = "ratings")
# p.als <- predict(r.als, getData(eval_small, "known"), type = "ratings")

# View sample predicted rating matrix
getRatingMatrix(p.svd)[1:6, 1:6]



getRatingMatrix(p.svdf)[1:6,1:6]

getRatingMatrix(p.als)[1:6,1:6]

```

```{r }

#Calculate the error between training prediction and unknown test data
error <- rbind(SVD = calcPredictionAccuracy(p.svd, getData(eval_small, "unknown")),
               SVDF = calcPredictionAccuracy(p.svdf, getData(eval_small, "unknown")),
               ALS = calcPredictionAccuracy(p.als, getData(eval_small, "unknown")))
kable(as.data.frame(error))

```


The prediction error results show that SVD achieves the lowest error across all metrics (RMSE, MSE, and MAE), indicating it provides the most accurate predictions on unseen test data. In contrast, FunkSVD (SVDF) and ALS exhibit higher errors, suggesting they are less effective in capturing the latent structure of user preferences in this dataset. This reinforces the strength of standard SVD in handling sparse rating matrices.


<a id="mySection55"></a>
<font size="4">
<B>
 User Based Collaborative Filtering (UBCF) Model
</B>
<font>

User Based Collaborative Filtering (UBCF) Model

User-Based Collaborative Filtering (UBCF) recommends items to a user based on the preferences of similar users. It identifies users with similar rating patterns using a similarity metric, and predicts ratings by aggregating the opinions of these nearest neighbors. This method works well when there is sufficient user overlap but can struggle with sparse data or new users (cold start problem).

Building a user-based collaborative filtering model in order to compare SVD model against other models.

```{r }
# Define train/test/known/unknown using the correct matrix
set.seed(123)
eval_scheme <- evaluationScheme(movieRealMatrix, method = "split", train = 0.8, given = -1)
train <- getData(eval_scheme, "train")
known <- getData(eval_scheme, "known")
unknown <- getData(eval_scheme, "unknown")

# UBCF model
library(tictoc)
tictoc::tic("UBCF Model - Training")
modelUBCF <- Recommender(train, method = "UBCF")
tictoc::toc(log = TRUE, quiet = TRUE)

tictoc::tic("UBCF Model - Predicting")
predUBCF <- predict(modelUBCF, newdata = known, type = "ratings")
tictoc::toc(log = TRUE, quiet = TRUE)

# Accuracy calculation
( accUBCF <- calcPredictionAccuracy(predUBCF, unknown) )

```
It leveraged user similarity to predict missing ratings. The accuracy results (e.g., RMSE, MAE) suggest that UBCF provides reasonably good recommendations, though potentially less accurate than matrix factorization techniques like SVD or ALS.

UBCF is computationally efficient for smaller datasets but may face challenges with scalability and sparsity in larger systems. Its performance serves as a useful baseline for evaluating more complex models.


<a id="mySection6"></a>
<font size="4">
<B>
 Data Visualization
</B>
<font>

Show the histogram of Movie data

```{r }
image(movieMatrix[1:100,1:100])
```

The visualization of the movie data matrix reveals the distribution and sparsity of user ratings across movies. The plot shows many blank or lightly colored areas, indicating a high level of missing ratings typical in recommender datasets.


```{r }
image(movieRealMatrix, main = "Raw Movie Data")
```

The heatmap of the raw movie rating matrix clearly illustrates the inherent sparsity of the dataset, where most users have rated only a small subset of movies. The scattered pattern of filled cells indicates that many user-movie combinations lack ratings, which is common in real-world recommender system data. This sparsity poses challenges for recommendation algorithms, making it essential to use techniques that can effectively handle missing data and uncover latent user preferences.


```{r }
image(normalize(movieRealMatrix), main = "Normalized Movie Data")
```

The heatmap of the normalized movie rating matrix shows a more balanced distribution of ratings across users. Normalization reduces user rating bias by centering ratings, allowing the model to focus on relative preferences rather than absolute scores. This step improves the effectiveness of matrix factorization methods by enhancing the signal in the sparse data.



```{r }
library(ggplot2)
#distribution of ratings
rating_frq <- as.data.frame(table(Ratings$rating))
ggplot(rating_frq,aes(Var1,Freq)) +   
  geom_bar(aes(fill = Var1), position = "dodge", stat="identity",fill="palegreen")+ labs(x = "Score")
```



```{r }
#distribution of rating mean of users
user_summary <-  as.data.frame(cbind( 'mean'=rowMeans(movieRealMatrix),'number'=rowCounts(movieRealMatrix)))
user_summary <-as.data.frame(sapply(user_summary, function(x) as.numeric(as.character(x))))
par(mfrow=c(1,2))
ggplot(user_summary,aes(mean)) +
  geom_histogram(binwidth = 0.05,col='white',fill="plum") + labs(x = "User Average Score")+geom_vline(xintercept = mean(user_summary$mean),col='grey',size=2)
```


```{r }
ggplot(user_summary,aes(number)) +
  geom_histogram(binwidth = 0.8,fill="plum") + labs(x = "Number of Rated Items")
```

```{r }
#distribution of rating mean of items
item_summary <- as.data.frame(cbind('mean'=colMeans(movieRealMatrix), 'number'=colCounts(movieRealMatrix)))
item_summary <-as.data.frame(sapply(item_summary, function(x) as.numeric(as.character(x))))
par(mfrow=c(1,2))
ggplot(item_summary,aes(mean)) +
  geom_histogram(binwidth = 0.05,col='white',fill="sandybrown") + 
  labs(x = "Item Average Score")+
  geom_vline(xintercept = mean(item_summary$mean),col='grey',size=2)
```

```{r }
ggplot(item_summary,aes(number)) +
  geom_histogram(binwidth = 0.8,fill="sandybrown") + labs(x = "Number of Scores Item has")
```

```{r }
m.cross <- evaluationScheme(movieRealMatrix[1:300], method = "cross", k = 4, given = 3, goodRating = 5)
m.cross.results.svd <- evaluate(m.cross, method = "SVD", type = "topNList", n = c(1, 3, 5, 10, 15, 20))
```


```{r }
par(mfrow=c(1,2))
plot(m.cross.results.svd, annotate = TRUE, main = "ROC Curve for SVD")
plot(m.cross.results.svd, "prec/rec", annotate = TRUE, main = "Precision-Recall for SVD")
```



<a id="mySection7"></a>
<font size="4">
<B>
 Singular Value Decomposition (SVD) Model
</B>
<font>



<a id="mySection71"></a>
<font size="4">
<B>
 How SVD Connects to Recommender Systems
</B>
<font>

SVD is a powerful matrix factorization technique that helps reduce the dimensionality of the user-item rating matrix by capturing latent factors—hidden features that explain the observed interactions. In the context of recommender systems, this is crucial because the original matrix is highly sparse: most users rate only a small fraction of available items.
 
By decomposing the original matrix \( A \) into three matrices:
 
\[
A \approx U_k \Sigma_k V_k^T
\]
 
we isolate the most informative patterns in user behavior and item characteristics. The matrices \( U_k \) and \( V_k \) represent users and items in a shared latent space, while \( \Sigma_k \) contains the singular values that reflect the strength of each latent factor.
 
This reduced representation allows us to:
- Predict missing ratings by reconstructing the approximated matrix.
- Handle sparsity more effectively than traditional collaborative filtering methods.
- Capture nuanced user preferences and item similarities not explicitly labeled in the data.
 
Thus, SVD serves as a collaborative filtering approach by leveraging patterns in the ratings data, even when explicit user or item attributes are not available.
 
Singular Value Decomposition matrix factorization method is implemented here in the context of a recommender system. This is done in two ways:
 
- Using SVD to estimate similarity
- Using SVD to create a content-based recommender
 
SVD begins by breaking an M by N matrix \( A \) (in this case M users and N movies) into the product of three matrices: \( U \), \( \Sigma \), and \( V^T \), where:
 
\[
A = U \Sigma V^T
\]
 
The matrix \( \Sigma \) is diagonal, with singular values representing the importance of each latent factor. Often, only the top \( k \) singular values and corresponding vectors in \( U \) and \( V \) are retained to form:
 
\[
A \approx U_k \Sigma_k V_k^T
\]
 
This dimensionality reduction approximates the original matrix while significantly reducing computation and noise.
 
 
<a id="mySection72"></a>
<font size="4">
<B>
SVD Model Implementation
</B>
<font>

 
```{r}
tictoc::tic("SVD Model - Training")
modelSVD <- Recommender(train, method = "SVD", parameter = list(k = 100))
tictoc::toc(log = TRUE, quiet = TRUE)
 
tictoc::tic("SVD Model - Predicting")
predSVD <- predict(modelSVD, newdata = known, type = "ratings")
tictoc::toc(log = TRUE, quiet = TRUE)
 
( accSVD <- calcPredictionAccuracy(predSVD, unknown) )
```

The SVD model was trained and predicted efficiently, showing good accuracy with low error metrics. This highlights SVD’s strong performance in both speed and prediction quality.



<a id="mySection73"></a>
<font size="4">
<B>
Run-Times
</B>
<font>

Compare the run time of various Factorization calculations

```{r }
# Display log as table
log <- as.data.frame(unlist(tictoc::tic.log(format = TRUE)))
colnames(log) <- c("Run Time")

knitr::kable(log, booktabs = TRUE) %>%
  kableExtra::kable_styling(latex_options = c("striped", "hold_position"))

```
The run-time analysis highlights the computational trade-offs between different recommendation algorithms. SVD typically has a longer training time due to matrix decomposition, but faster prediction speed once trained. In contrast, UBCF is faster to train but can be slower in generating predictions, especially for large user bases. ALS falls somewhere in between, depending on implementation and dataset density.

These results emphasize that while accuracy is important, efficiency and scalability are also critical when choosing a model for deployment in real-world systems.

<a id="mySection74"></a>
<font size="4">
<B>
Predictions
</B>
<font>

Predict popular movies based on high ratings for user#44


```{r }
# Convert rating matrix to data frame
rating_matrix_df <- as.data.frame(as.matrix(getRatingMatrix(movieRealMatrix)))

# Extract user 44's ratings
user_ratings <- rating_matrix_df["44", , drop = TRUE]  # drop = TRUE gives a named vector

# Convert to data frame with movieId and Rating
movie_rated <- data.frame(
  movieId = as.integer(names(user_ratings)),
  Rating = as.numeric(user_ratings)
)

# Filter, join with movie titles, and arrange
movie_rated <- movie_rated %>%
  filter(Rating != 0) %>%
  inner_join(Movies, by = "movieId") %>%
  arrange(desc(Rating)) %>%
  select(Movie = title, Rating)

# Display the rated movies
knitr::kable(head(movie_rated,15), booktabs = TRUE, caption = "Movies Rated by User 44") %>%
  kableExtra::kable_styling(latex_options = c("striped", "hold_position"))

```


Identify recommended movies for user#44

```{r }
# Check if user 44 is in the prediction
if ("44" %in% rownames(getRatingMatrix(predSVD))) {
  
  movie_recommend <- as.data.frame(as.matrix(getRatingMatrix(predSVD)["44", , drop = FALSE]))
  colnames(movie_recommend) <- c("Rating")
  movie_recommend$movieId <- as.integer(rownames(movie_recommend))
  
  movie_recommend <- movie_recommend %>% 
    arrange(desc(Rating)) %>%
    filter(Rating > 0) %>%
    head(6) %>%
    inner_join(Movies, by = "movieId") %>%
    select(Movie = title)
  
  knitr::kable(movie_recommend, booktabs = TRUE, caption = "Top Recommendations for User 44") %>%
    kableExtra::kable_styling(latex_options = c("striped", "hold_position"))
  
} else {
  print("_")
}


```

Movie

Home Alone (1990)

Dragonheart (1996)

Jurassic Park (1993)

Contact (1997)

Lion King, The (1994)

Spider-Man 2 (2004)


The list of top-rated movies by User #44 reflects their personal preferences and rating tendencies, which form the basis for generating personalized recommendations. These high-rated movies help the model infer latent interests and match them with similar items in the catalog. This user-centric view is essential in collaborative filtering systems, as it allows for targeted and relevant recommendations.


<a id="mySection75"></a>
<font size="4">
<B>
Manual Singular Value Decomposition
</B>
<font>



Trying to build the SVD model without the use of recommender package functionality and instead using the base R provided svd function

First the movie ratings matrix is normalized. NA values are replaced with 0 and there are negative and positive ratings. Now we can decompose original matrix.

```{r }
# Normalize the matrix (replace NAs with 0 and normalize ratings)
movieMatrix <- as.matrix(normalize(movieRealMatrix)@data)

# Perform SVD
movieSVD <- svd(movieMatrix)
rownames(movieSVD$u) <- rownames(movieMatrix)
rownames(movieSVD$v) <- colnames(movieMatrix)

# Extract singular values
Sigmak <- movieSVD$d 
Uk <- movieSVD$u 
Vk <- t(as.matrix(movieSVD$v))

# Calculate cumulative proportion of variance explained
cumulative_variance <- cumsum(Sigmak^2) / sum(Sigmak^2)

# Plot the cumulative variance explained by the first n components
plot(cumulative_variance, type = "b", pch = 19, col = "blue", 
     xlab = "Number of Components", ylab = "Cumulative Variance Explained",
     main = "Cumulative Variance Explained by SVD Components")

# Add lines for 80% and 90% of the variance explained
abline(h = 0.80, col = "red", lty = 2)
abline(h = 0.90, col = "green", lty = 2)

# Annotate the points where 80% and 90% are reached
text(which(cumulative_variance >= 0.80)[1], 0.80, labels = "80%", pos = 4, col = "red")
text(which(cumulative_variance >= 0.90)[1], 0.90, labels = "90%", pos = 4, col = "green")

```


To estimate the value of k, the cumulative proportion of the length of the vector d represented by the set of items running through an index n is calculated and plotted. The values of n at which 80% and 90% of the vector’s length is included are found and plotted:

```{r }
# Convert realRatingMatrix to a regular matrix
ratingmat <- as(movieRealMatrix, "matrix")

ratingmat[is.na(ratingmat)] <- 0
ratingmat[is.infinite(ratingmat)] <- 0
ratingmat <- ratingmat[1:100, 1:100]

s <- svd(ratingmat)

k <- 20
U_k <- s$u[, 1:k]
D_k <- diag(s$d[1:k])
V_k <- s$v[, 1:k]

predmat <- U_k %*% D_k %*% t(V_k)

# Plot (adjust size if matrix has fewer rows/columns)
par(mfrow = c(1, 2))  # side-by-side plots
image(ratingmat, main = "Original Ratings Matrix")
image(predmat, main = "Reconstructed Ratings Matrix (SVD)")


```

The plot shows the descending order of the singular values quite clearly, with the magnitudes declining rapidly through the first 30 or so singular values before leveling out at a magnitude of somewhat less than 200.

```{r }
# Reduce dimensions
n <- length(movieSVD$d)
total_energy <- sum(movieSVD$d^2)
for (i in (n-1):1) {
  energy <- sum(movieSVD$d[1:i]^2)
  if (energy/total_energy<0.9) {
    n_dims <- i+1
    break
  }
}

trim_mov_D <- movieSVD$d[1:n_dims]
trim_mov_U <- movieSVD$u[, 1:n_dims]
trim_mov_V <- movieSVD$v[, 1:n_dims]
```


```{r }
trimMovies <- as.data.frame(trim_mov_V) %>% select(V1, V2)
trimMovies$movieId <- as.integer(rownames(trimMovies))

movieSample <- trimMovies %>% arrange(V1) %>% head(5)
movieSample <- rbind(movieSample, trimMovies %>% arrange(desc(V1)) %>% head(5))
movieSample <- rbind(movieSample, trimMovies %>% arrange(V2) %>% head(5))
movieSample <- rbind(movieSample, trimMovies %>% arrange(desc(V2)) %>% head(5))

movieSample <- movieSample %>%
  inner_join(Movies, by = "movieId") %>%
  select(Movie = title, Concept1 = V1, Concept2 = V2)

movieSample$Concept1 <- round(movieSample$Concept1, 4)
movieSample$Concept2 <- round(movieSample$Concept2, 4)

knitr::kable(movieSample, booktabs = TRUE, caption = "Top and Bottom Movies by SVD Concept Values") %>%
  kableExtra::kable_styling(latex_options = c("striped", "hold_position"))

 
```

```{r }
# Plot labels without overlapping
library(ggrepel)
```

```{r }
ggplot(movieSample, aes(Concept1, Concept2, label=Movie)) + 
  geom_point() +
  geom_text_repel(aes(label=Movie), hjust=-0.1, vjust=-0.1, size = 3) +
  scale_x_continuous(limits = c(-0.2, 0.2)) +
  scale_y_continuous(limits = c(-0.1, 0.1)) 
```



The above plot demonstrates one of the disadvantages of the SVD method - inability to connect characteristics/concepts to real-world categories. As can be seen that all original Star Wars movies are close together or that Pulp Fiction and Ace Ventura are on opposites sides, but there is no clear way to categorize these movies.



<a id="mySection8"></a>
<font size="4">
<B>
Empirical Results:
</B>
<font>



Why SVD Performed Best
In your experiment, the SVD model achieved the lowest RMSE (0.872), lowest MSE (0.760), and lowest MAE (0.658) among all models tested:

          Model	  RMSE	  MSE	    MAE
          ============================
          SVD	    0.8720	0.7604	0.6587
          SVDF	  1.1683	1.3649	0.7936
          ALS	    0.9166	0.8402	0.7097
          UBCF	  1.2667	1.6045	1.0476

This confirms that SVD not only compresses the data effectively but also makes accurate predictions, making it highly suitable for recommender systems.

Singular Value Decomposition (SVD) plays a central role in recommender systems by addressing the challenge of sparse user-item matrices through dimensionality reduction and latent factor modeling. By decomposing the original matrix into user and item factors, SVD uncovers hidden relationships—such as user preference patterns or item similarities—that are not explicitly available. This enables the system to predict missing ratings with greater accuracy, improving recommendation quality. As shown in the results, SVD achieved the lowest RMSE (0.8720) and MAE (0.6587) compared to FunkSVD, ALS, and UBCF, confirming its effectiveness in capturing these underlying structures.

FunkSVD, while designed for recommender systems, performed worse likely due to suboptimal tuning or convergence issues during training. ALS, though effective for large-scale systems, can struggle with precision on smaller or denser datasets due to its alternating optimization approach. UBCF, a neighborhood-based method, relies heavily on user similarity and performs poorly in sparse datasets where users have few overlapping ratings, leading to less reliable predictions.



<a id="mySection9"></a>
<font size="4">
<B>
Conclusion
</B>
<font>


Observations are as follows

From the current sample, it seems centered data with base SVD outperforms
SVD predictions are significantly faster compared to UBCF at the initial cost of time taken to build the training model
SVD interpretation was a bit confusing and difficult
In this dataset SVDF and SVD are showing providing similar results

This analysis show that SVD-based models outperform user-based collaborative filtering (UBCF) in terms of prediction accuracy, particularly when the data is mean-centered. While SVD and SVDF delivered comparable results, SVD stood out for its balance between accuracy and prediction speed, despite longer initial training times. ALS, though slightly less accurate in this case, proved valuable for handling sparse matrices and could be further optimized.

We also noted that matrix factorization methods, especially SVD, effectively reduce dimensionality and capture latent user preferences, though their interpretability remains limited. Overall, matrix factorization demonstrates strong potential for scalable, accurate recommendation systems, and future enhancements could include hybrid approaches or deeper tuning of model parameters.




