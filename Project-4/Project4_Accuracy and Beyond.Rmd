---
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
   
---

<div style="text-align:center;"> 
  <h1><B>DATA 612 - Project 4 : Accuracy and Beyond </B></h1>
  <h3><B>Bikash Bhowmik, Rupendra Shrestha</B></h3> 
  <h4><B>29 Jun 2025</B></h4>
</div>


Column {data-width=150}
-----------------------------------------------------------------------

### 
<a href="#mySection1" style="font-size: 1em; font-weight: bold;" >Instruction</a>
<br>
<a href="#mySection2" style="font-size: 1em; font-weight: bold;" >Introduction</a>
<br>
<a href="#mySection3" style="font-size: 1em; font-weight: bold;" >Load Data</a>
<br>
<a href="#mySection4" style="font-size: 1em; font-weight: bold;" >Data Cleansing</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection41" style="font-size: 1em; " >Data Exploration</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection42" style="font-size: 1em; " >Genre Exploration</a>
<br>
<a href="#mySection5" style="font-size: 1em; font-weight: bold;" >Initial Recommender Systems</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection51" style="font-size: 1em; " >Parameters</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection52" style="font-size: 1em; " >Creation of Systems</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection53" style="font-size: 1em; " >Comparisons</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection54" style="font-size: 1em; " >Model Tuning</a>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#mySection55" style="font-size: 1em; " >Final Model</a>
<br>
<a href="#mySection6" style="font-size: 1em;font-weight: bold; " >Introduce Novelty</a>
<br>
<a href="#mySection7" style="font-size: 1em;font-weight: bold; " >Compare Results</a>
<br>
<a href="#mySection8" style="font-size: 1em;font-weight: bold; " >Discussion</a>
<br>



Column {data-width=875}
-----------------------------------------------------------------------
### 
<a id="mySection1"></a> 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Load all required packages
library(readxl)
library(recommenderlab)
library(ggplot2)
```

<font size="4">
<B>
Instruction
</B>
<font>

The goal of this assignment is give you practice working with accuracy and other recommender system metrics.In this assignment you’re asked to do at least one or (if you like) both of the following:

Work in a small group, and/or
Choose a different dataset to work with from your previous projects.
Deliverables
As in your previous assignments, compare the accuracy of at least two recommender system algorithms against your offline data.

Implement support for at least one business or user experience goal such as increased serendipity, novelty, or diversity.

Compare and report on any change in accuracy before and after you’ve made the change in #2.

As part of your textual conclusion, discuss one or more additional experiments that could be performed and/or metrics that could be evaluated only if online evaluation was possible. Also, briefly propose how you would design a reasonable online evaluation environment.


<a id="mySection2"></a>
<font size="4">
<B>
Introduction
</B>
<font>

The project at hand is to build a recommender system that can provide users with personalized joke recommendations. In addition to optimizing for recommendations that are accurate, the recommender also focuses on maximizing user experience by adding some level of serendipity to the recommendations - meaning users will receive unexpected items with the potential to be enjoyable within their most highly recommended items. 

Initially, we investigate the performance of five recommendation algorithms:

+ User-Based Collaborative Filtering (UBCF) using cosine and Pearson similarity

+ Item-Based Collaborative Filtering (IBCF) using cosine and Pearson similarity

+ A Random recommender algorithm (a baseline)

For each method, we conduct a 3-fold cross-validation to evaluate precision and performance. After we establish the best performing algorithm, we promote novel items in the recommendation list by replacing a selection of the most recommended items with more novel items. In this approach we try to balance accurate recommendations with user engagement and discovery.

<a id="mySection3"></a>
<font size="4">
<B>
Load Data
</B>
<font>

We use the Jester dataset, which includes joke ratings from 24,983 users. Each user has rated at least 36 jokes, with rating values ranging from -10.00 (least funny) to +10.00 (most funny). A rating of 99 indicates that a joke was not rated by the user. In the dataset, each row corresponds to a single user: the first column records the total number of jokes they rated, while the next 100 columns contain their individual ratings for each joke.

```{r}
# Load the jester dataset
library(readxl)

# Read the Excel file (first sheet)
jester <- data.frame(read_xls("jester-data-1.xls", col_names = FALSE))

# Assign column names: first column is rating count, followed by 100 joke ratings
##colnames(jester) <- c("ratingCount", paste0("Joke_", 1:100))
##row.names(jester) <- 1:nrow(jester)

# Display the structure and first few rows of the data
# str(jester)
head(jester[, 1:6])  # Show the first 5 columns (rating count + 5 jokes)

# Basic summary statistics for joke ratings
summary(jester[, 2:6])

```
This gives a quick sense of how the data is structured and how ratings are distributed across the first few jokes. It also confirms that the dataset has the correct dimensions and types before moving on to data preprocessing.

<a id="mySection4"></a>
<font size="4">
<B>
Data Cleansing
</B>
<font>

First, we will load in the jester data set. We will remove the column with the number of rated jokes because this will not be used in the recommendation system. Additionally, the raw data represents non-rated jokes as the number 99, so we will replace these values with nulls. Finally, we will subset the data to 5,000 users to speed up computation time.

```{r }
# Read jester data
##jester <- data.frame(read_xls("jester-data-1.xls", col_names = FALSE))
colnames(jester) <- c("ratingCount", seq(100))
row.names(jester) <- 1:nrow(jester)

# remove num jokes column
ratings <- jester[-1]

# replace 0 (no rating) with NULL
ratings[ratings == 99] <- NA
ratings <- ratings[1:5000,]
ratings <- as.matrix(ratings)

# Create large dgCMatrix
finalRatings <- as(ratings, 'realRatingMatrix')
```

<a id="mySection41"></a>
<font size="4">
<B>
 Data Exploration
</B>
<font>

Let’s dive a little deeper into our data.

First, let’s take a look at the number of jokes that each user has rated. We set the threshold at 36 jokes, and it appears that most individuals have rated either around 70 or 100 jokes.

```{r }
jokeCount <- rowCounts(finalRatings)
hist(jokeCount,
     main = 'Number of Jokes Rated per User',
     xlab = 'Number of Jokes Rated',
     ylab = 'Number of Users')

```

Next, we can look at the number of ratings that each joke has. We can see that many of the jokes were rated by all 5000 users.



```{r }
ratingCount <- colCounts(finalRatings)
hist(ratingCount,
     main = 'Number of Individuals Rating each Joke',
     xlab = 'Number of Users that Rated Joke',
     ylab = 'Number of Jokes') 
```

Now we can take a look at the average rating across all jokes. The median average rating is a little over 0 (neutral), which means that jokes are typically rated with a small positive skew.


```{r }
# average rating
mean_rating <- colMeans(finalRatings, na.rm = T)
quantile(mean_rating)
```

We can also plot the average ratings. A look at the distribution shows that most jokes have an average rating between -2 and 3. We have a few outliers that are rated more positively (+4) and more negatively (-4).


```{r }
goodrating <- quantile(mean_rating, .5)
qplot(mean_rating) + ggtitle("Distribution of Average Joke Rating") + geom_vline(xintercept = goodrating, col='red')
```

From this data exploration, we can see that most users rated all jokes and conversely, most jokes were rated by all users. The median average rating for the users is a little over 0, so we will use 1 as our threshold for a good joke.


<a id="mySection42"></a>
<font size="4">
<B>
 Genre Exploration
</B>
<font>



```{r }
### Genre Exploration 

library(readxl)
library(ggplot2)

# Load the Jester dataset
jester <- read_excel("jester-data-1.xls", col_names = FALSE)

# Remove first column (rating count) and replace 99s with NA
ratings <- jester[ , -1]
ratings[ratings == 99] <- NA

# Summary of ratings
summary(ratings)

# Histogram: how many jokes each user rated
hist(rowSums(!is.na(ratings)),
     main = "Number of Jokes Rated per User",
     xlab = "Jokes Rated", ylab = "Users")

# Histogram: how many users rated each joke
hist(colSums(!is.na(ratings)),
     main = "Number of Users per Joke",
     xlab = "Users per Joke", ylab = "Jokes")

# Histogram: average joke ratings
avg_joke_rating <- colMeans(ratings, na.rm = TRUE)
hist(avg_joke_rating,
     main = "Average Rating per Joke",
     xlab = "Avg Rating", ylab = "Jokes")


```

We explored the distribution of movie genres in the dataset. This helps evaluate whether the recommender covers a wide range of genres or mostly sticks to popular ones.





<a id="mySection5"></a>
<font size="4">
<B>
Initial Recommender Systems
</B>
<font>


<a id="mySection51"></a>
<font size="4">
<B>
Parameters
</B>
<font>

We’ll define the following:

+ Training Percent: The percent of the data that should be used in training. The remaining data will be used for testing.

+ Items To Keep: The total number of items that will be used to generate the recommendations. The remaining items will be used to test the model                accuracy. We’ll identify the min number of jokes that an individual has rated and use a few less than that.

+ Rating Threshold: The threshold to be used for positive ratings. Since our data is on a scale of -10 to 10, we will use 1 as the threshold for a good joke.

+ Number of Folds: This is the number of folds that will be used for k-fold validation.

Finally, we’ll define our evaluation scheme for the models.

```{r }
trainPct <- 0.8
toKeep <- min(rowCounts(finalRatings)) - 5
ratingThreshold <- 1
nFold <- 3

# define evaluation scheme
evalScheme <- evaluationScheme(finalRatings, 
                               method = "cross-validation", 
                               k = nFold, 
                               given = toKeep, 
                               goodRating = ratingThreshold)
```


<a id="mySection52"></a>
<font size="4">
<B>
Creation of Systems
</B>
<font>

Now that we’ve set up the evaluation scheme for our recommender systems, we can compare different models. We will evaluate the output of 2 IBCF models (using cosine and pearson similarities), 2 UBCF models (once again, using cosine and pearson similarities), and 1 random model for a baseline. We will also vary the number of recommendations from 5 to 20.


```{r }
# models to compare
evalModels <- list(
 IBCF_cos = list(name = "IBCF", param = list(method =
 "cosine")),
 IBCF_cor = list(name = "IBCF", param = list(method =
 "pearson")),
 UBCF_cos = list(name = "UBCF", param = list(method =
 "cosine")),
 UBCF_cor = list(name = "UBCF", param = list(method =
 "pearson")),
 RANDOM = list(name = "RANDOM", param = NULL)
)

# number of recommendations
nRecs <- c(1, seq(5, 20, 5))

finalResults <- evaluate(x = evalScheme, method = evalModels, n = nRecs)

```



<a id="mySection53"></a>
<font size="4">
<B>
Comparisons
</B>
<font>

We can look at the average results across all folds for each algorithm. Each row represents a different number of recommendations. We can see that on average, as the number of recommendations increases, so does our accuracy.

```{r }
avgs <- lapply(finalResults, avg)
avgs

```

We can also visualize the ROC curves for each of the algorithms we’ve run. Each marker on the graph represents the TP/FP ratio for n
 recommendations. The plot shows higher performance from the UBCF models.
 
```{r }
plot(finalResults, annotate = 1, legend = "topleft")
title("ROC curve")
```

The goal of our recommender system is to provide jokes that are funny to a user, so we want to minimize the number of false positives (recommendations that are wrong). We will therefore choose the algorithm with the highest precision (true positive rate). Once again, the UBCF models outperform the others.

```{r }
plot(finalResults, "prec/rec", annotate = 1, legend = "bottomright")
title("Precision-recall")
```

<a id="mySection54"></a>
<font size="4">
<B>
 Model Tuning
</B>
<font>

Based on this analysis, we will choose the UBCF model with Pearson similarity and 5 recommendations. We can further tune nn parameter for the the model.

```{r }
numNeighbors <- seq(100,200,50)

ubcfModels <- lapply(numNeighbors, function(n){
 list(name = "UBCF", param = list(method = "pearson", nn = n))
})
names(ubcfModels) <- paste0("UBCF_", numNeighbors)

ubcfSchemes <- evaluate(x = evalScheme, method = ubcfModels, n = 5)

```
We’ll pick the model with the best precision, which is 200 neighbors:

```{r}
avg(ubcfSchemes)

```

<a id="mySection55"></a>
<font size="4">
<B>
 Final Model
</B>
<font>

Now, we can define our final model and calculate the precision and RMSE:

```{r }
set.seed(200)

# UBCF Model
ubcfRec <- Recommender(getData(evalScheme, "train"), 'UBCF', parameter = list(method = 'pearson', nn = 200, normalize = 'center'))
ubcfPredN <- predict(ubcfRec, getData(evalScheme, "known"), n = 5)
ubcfPredR <- predict(ubcfRec,getData(evalScheme,'known'), type = 'ratings')

# calc accuracy on test set
ubcfAccN <- calcPredictionAccuracy(ubcfPredN, 
                                   getData(evalScheme, "unknown"), 
                                   given = toKeep, 
                                   goodRating = ratingThreshold)

ubcfAccR <- calcPredictionAccuracy(ubcfPredR, getData(evalScheme, "unknown"))

ubcfAccN
```


```{r }
ubcfAccR
```



<a id="mySection6"></a>
<font size="4">
<B>
Introduce Novelty
</B>
<font>

In order to introduce novelty to the recommendations, we’ll take a percentage of the top recommendations from the UBCF model and switch the recommendations out with randomly selected jokes. To do this, we’ll define a recommendation system using the RANDOM methodology and then create a hybrid recommender that combines it with the UBCF model.


```{r }
# Random Model
randRec <- Recommender(getData(evalScheme, "train"), 'RANDOM')

hybridRec <- HybridRecommender(ubcfRec,randRec, weights = c(0.8,0.2))
hybridPredN <- predict(hybridRec, getData(evalScheme, "known"), n = 5)
hybridPredR <- predict(hybridRec, getData(evalScheme,'known'), type = 'ratings')

# calc accuracy on test set
hybridAccN <- calcPredictionAccuracy(hybridPredN, 
                                     getData(evalScheme, "unknown"), 
                                     given = toKeep, 
                                     goodRating = ratingThreshold)
hybridAccR <- calcPredictionAccuracy(hybridPredR, getData(evalScheme, "unknown"))

hybridAccN
```



```{r }
hybridAccR
```

<a id="mySection7"></a>
<font size="4">
<B>
Compare Results
</B>
<font>


We can compare the results of the UBCF-only model vs the hybrid model. The precision is worse in the hybrid model and the RMSE is higher.
 


```{r}
data.frame(METHOD = c('UBCF','HYBRID'), 
           PRECISION = c(ubcfAccN['precision'], hybridAccN['precision']),
           RMSE = c(ubcfAccR['RMSE'], hybridAccR['RMSE']))
```

We can also take a look at a comparison of the top 5 suggestions for one of the users. From this, we can see two things:

The ordering has changed in the recommendations.

There are new items in the hybrid system, which represent the random recommendations.

```{r}
hybridPredN@items[2]
```

```{r}
ubcfPredN@items[2]
```


<a id="mySection8"></a>
<font size="4">
<B>
Discussion
</B>
<font>

In this project, we evaluated several recommender system algorithms using offline metrics such as precision and RMSE to assess accuracy and effectiveness. These metrics guided our selection of User-Based Collaborative Filtering (UBCF) with Pearson similarity as a strong baseline model. To support user experience goals like novelty and diversity, we introduced a Hybrid Recommender that blends UBCF with a Random model.

In the final phase, we narrowed our focus to these two models—UBCF and the Hybrid—and concentrated on precision as our primary evaluation metric, as it most directly reflects the relevance of top-N recommendations. This focused comparison allowed us to assess the trade-off between pure accuracy and the added value of introducing novelty.

While offline evaluation provides meaningful insights, it cannot fully capture how users engage with recommendations in real-world applications. For a more complete understanding, online metrics such as click-through rate (CTR), dwell time, and user retention are crucial. An ideal future evaluation environment would use A/B testing with tools like Google Optimize or Firebase, and user behavior tracking with platforms like Mixpanel or Amplitude. This would enable a more realistic, data-driven refinement of recommendation strategies tailored to user satisfaction and business impact.



